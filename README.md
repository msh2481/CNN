# Sirius-SPbSU-2022: entry competition
## Что происходило до смены правил
Сначала я не планировал никуда отбираться и отчёт об экспериментах не готовил, поэтому перескажу примерный порядок по памяти.
1. Попробовал `torchvision.models.resnet18` с Adam или SGD и слегка подобранными гиперпараметрами. ~87%
2. Сгладил чёрно-белые точки в исходных данных. ~89%
3. Вставил `nn.Dropout` между всеми слоями, кажется не помогло. 
4. Добавил `torchvision.transforms.AutoAugment` в обучение. ~91%
5. Попробовал делать эту же аугментацию несколько раз при валидации и тестировании чтобы получить что-то вроде ансамбля, не помогло.
6. Добавил все архитектуры из https://arxiv.org/abs/2008.10400, на входе добавил `CenterCrop(28)`, чтобы по возможности не подбирать архитектуру заново. ~92%
7. Попробовал ансамбли из раннее обученных моделей, двух видов: где модели просто голосуют, и где они соединяются дополнительным линейным слоем. ~93%
8. В какой-то момент перешёл на QHAdam, потому что он точно не хуже SGD и Adam при правильно подобранных параметрах, но пока не успел эти параметры перебрать.
9. Попробовал tianshou.PrioritizedReplayBuffer (в оригинальной статье писали, что на MNIST помогает), стало вдруг дико переобучаться несмотря на агументацию.
10. Добавил cutout (вырезание квадратов из изображения), но не успел настроить его до нужной силы регуляризации.
## Собственно отчёт
Заменил `Autoaugment` на отдельные преобразования, добавил параметры к cutout.
По-новой сгладил данные, на этот раз получилось лучше, шума совсем не видно.
Наконец сделал нормальный перебор гиперпараметров, для начала просто случайный поиск, распределения такие (`norm_rnd(mean, std, clip_min, clip_max)`):

    cutout_min = norm_rnd(4, 4, 0, 16)
    bs = 2**randint(5, 10)

    'jitter_brightness': norm_rnd(0, 0.1, 0, 0.5),
    'jitter_contrast': norm_rnd(0, 0.1, 0, 0.5),
    'jitter_saturation': norm_rnd(0, 0.1, 0, 0.5),
    'jitter_hue': norm_rnd(0, 0.1, 0, 0.5),
    'perspective_distortion': norm_rnd(0, 0.1, 0, 1),
    'cutout_count': int(norm_rnd(0, 1, 0, 10)),
    'cutout_min_size': int(cutout_min),
    'cutout_max_size': int(cutout_min * norm_rnd(2, 0.5, 1, 10)),

    'model': choice(['M5()', 'M7()', 'Resnet18(10)']),
    'batch_size': bs,

    'optimizer': 'QHAdam',
    'lr': 10**norm_rnd(-3, 1, -6, -1),
    'wd': 10**norm_rnd(-4, 1, -7, -2),
    'beta1': 0.9,
    'beta2': 0.999,
    'nu1': norm_rnd(0.5, 0.2, 0.1, 0.9),
    'nu2': norm_rnd(1, 0.1, 0.8, 1)
`
## TODO
Дистилляция чего-нибудь с чем-нибудь.
Optuna и другие оптимизаторы гиперпараметров.
Другие архитектуры.
Сглаживание весов.
Регуляризация через weight decay.
MaxNorm на веса или градиенты.
